{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from matplotlib import pylab\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import tensorflow as tf\n",
    "from scipy.sparse import lil_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file:  stories\\001.txt\n",
      "Downloading file:  stories\\002.txt\n",
      "Downloading file:  stories\\003.txt\n",
      "Downloading file:  stories\\004.txt\n",
      "Downloading file:  stories\\005.txt\n",
      "Downloading file:  stories\\006.txt\n",
      "Downloading file:  stories\\007.txt\n",
      "Downloading file:  stories\\008.txt\n",
      "Downloading file:  stories\\009.txt\n",
      "Downloading file:  stories\\010.txt\n",
      "Downloading file:  stories\\011.txt\n",
      "Downloading file:  stories\\012.txt\n",
      "Downloading file:  stories\\013.txt\n",
      "Downloading file:  stories\\014.txt\n",
      "Downloading file:  stories\\015.txt\n",
      "Downloading file:  stories\\016.txt\n",
      "Downloading file:  stories\\017.txt\n",
      "Downloading file:  stories\\018.txt\n",
      "Downloading file:  stories\\019.txt\n",
      "Downloading file:  stories\\020.txt\n",
      "Downloading file:  stories\\021.txt\n",
      "Downloading file:  stories\\022.txt\n",
      "Downloading file:  stories\\023.txt\n",
      "Downloading file:  stories\\024.txt\n",
      "Downloading file:  stories\\025.txt\n",
      "Downloading file:  stories\\026.txt\n",
      "Downloading file:  stories\\027.txt\n",
      "Downloading file:  stories\\028.txt\n",
      "Downloading file:  stories\\029.txt\n",
      "Downloading file:  stories\\030.txt\n",
      "Downloading file:  stories\\031.txt\n",
      "Downloading file:  stories\\032.txt\n",
      "Downloading file:  stories\\033.txt\n",
      "Downloading file:  stories\\034.txt\n",
      "Downloading file:  stories\\035.txt\n",
      "Downloading file:  stories\\036.txt\n",
      "Downloading file:  stories\\037.txt\n",
      "Downloading file:  stories\\038.txt\n",
      "Downloading file:  stories\\039.txt\n",
      "Downloading file:  stories\\040.txt\n",
      "Downloading file:  stories\\041.txt\n",
      "Downloading file:  stories\\042.txt\n",
      "Downloading file:  stories\\043.txt\n",
      "Downloading file:  stories\\044.txt\n",
      "Downloading file:  stories\\045.txt\n",
      "Downloading file:  stories\\046.txt\n",
      "Downloading file:  stories\\047.txt\n",
      "Downloading file:  stories\\048.txt\n",
      "Downloading file:  stories\\049.txt\n",
      "Downloading file:  stories\\050.txt\n",
      "Downloading file:  stories\\051.txt\n",
      "Downloading file:  stories\\052.txt\n",
      "Downloading file:  stories\\053.txt\n",
      "Downloading file:  stories\\054.txt\n",
      "Downloading file:  stories\\055.txt\n",
      "Downloading file:  stories\\056.txt\n",
      "Downloading file:  stories\\057.txt\n",
      "Downloading file:  stories\\058.txt\n",
      "Downloading file:  stories\\059.txt\n",
      "Downloading file:  stories\\060.txt\n",
      "Downloading file:  stories\\061.txt\n",
      "Downloading file:  stories\\062.txt\n",
      "Downloading file:  stories\\063.txt\n",
      "Downloading file:  stories\\064.txt\n",
      "Downloading file:  stories\\065.txt\n",
      "Downloading file:  stories\\066.txt\n",
      "Downloading file:  stories\\067.txt\n",
      "Downloading file:  stories\\068.txt\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.cs.cmu.edu/~spok/grimmtmp/'\n",
    "# Создание директории\n",
    "dir_name = 'stories'\n",
    "if not os.path.exists(dir_name):\n",
    "    os.mkdir(dir_name)\n",
    "\n",
    "# Скачивание файла\n",
    "def maybe_download(filename):\n",
    "  print('Downloading file: ', dir_name+ os.sep+filename)\n",
    "    \n",
    "  if not os.path.exists(dir_name+os.sep+filename):\n",
    "    filename, _ = urlretrieve(url + filename, dir_name+os.sep+filename)\n",
    "  else:\n",
    "    print('File ',filename, ' already exists.')\n",
    "  \n",
    "  return filename\n",
    "\n",
    "num_files = 100\n",
    "filenames = [format(i, '03d')+'.txt' for i in range(1,101)]\n",
    "\n",
    "for fn in filenames:\n",
    "    maybe_download(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "  \n",
    "  with open(filename) as f:\n",
    "    data = tf.compat.as_str(f.read())\n",
    "    data = data.lower()\n",
    "    data = list(data)\n",
    "  return data\n",
    "\n",
    "global documents\n",
    "documents = []\n",
    "for i in range(num_files):    \n",
    "    print('\\nProcessing file %s'%os.path.join(dir_name,filenames[i]))\n",
    "    chars = read_data(os.path.join(dir_name,filenames[i]))\n",
    "    two_grams = [''.join(chars[ch_i:ch_i+2]) for ch_i in range(0,len(chars)-2,2)]\n",
    "    documents.append(two_grams)\n",
    "    print('Data size (Characters) (Document %d) %d' %(i,len(two_grams)))\n",
    "    print('Sample string (Document %d) %s'%(i,two_grams[:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(documents):\n",
    "    chars = []  \n",
    "    for d in documents:\n",
    "        chars.extend(d)\n",
    "    print('%d Characters found.'%len(chars))\n",
    "    count = []\n",
    "    # Получение биграммы, сортированные по частоте встреч(сортировка по убыванию)\n",
    "    count.extend(collections.Counter(chars).most_common())\n",
    "    \n",
    "    # Создание словарь с ID для каждой биграммы\n",
    "    # 'UNK' означает слишком редкое слово\n",
    "    dictionary = dict({'UNK':0})\n",
    "    for char, c in count:\n",
    "        # Добавление в словарь только биграммы с частотой >10\n",
    "        if c > 10:\n",
    "            dictionary[char] = len(dictionary)    \n",
    "    \n",
    "    unk_count = 0\n",
    "    # Замена каждой биграммы на ее ID\n",
    "    data_list = []\n",
    "    for d in documents:\n",
    "        data = list()\n",
    "        for char in d:\n",
    "            # Если слово есть в словаре, используется ID,\n",
    "            # иначе ID специального токена \"UNK\"\n",
    "            if char in dictionary:\n",
    "                index = dictionary[char]        \n",
    "            else:\n",
    "                index = dictionary['UNK']\n",
    "                unk_count += 1\n",
    "            data.append(index)\n",
    "            \n",
    "        data_list.append(data)\n",
    "        \n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "    return data_list, count, dictionary, reverse_dictionary\n",
    "\n",
    "global data_list, count, dictionary, reverse_dictionary,vocabulary_size\n",
    "\n",
    "# Статистика о полученных данных\n",
    "data_list, count, dictionary, reverse_dictionary = build_dataset(documents)\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Least common words (+UNK)', count[-15:])\n",
    "print('Sample data', data_list[0][:10])\n",
    "print('Sample data', data_list[1][:10])\n",
    "print('Vocabulary: ',len(dictionary))\n",
    "vocabulary_size = len(dictionary)\n",
    "del documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGeneratorOHE(object):\n",
    "    \n",
    "    def __init__(self,text,batch_size,num_unroll):\n",
    "        # Текст, записанный в виде ID биграмм\n",
    "        self._text = text\n",
    "        # Число биграмм в тексте\n",
    "        self._text_size = len(self._text)\n",
    "        # Number of datapoints in a batch of data\n",
    "        self._batch_size = batch_size\n",
    "        # Число предыдущих шагов, для которых развернут вход\n",
    "        self._num_unroll = num_unroll\n",
    "        # Разбитие текста на несколько сегментов\n",
    "        self._segments = self._text_size//self._batch_size\n",
    "        self._cursor = [offset * self._segments for offset in range(self._batch_size)]\n",
    "        \n",
    "    def next_batch(self):\n",
    "        '''\n",
    "        Генерирование одиночного пакета данных\n",
    "        '''\n",
    "        # Тренировка входных и выходных данных\n",
    "        batch_data = np.zeros((self._batch_size,vocabulary_size),dtype=np.float32)\n",
    "        batch_labels = np.zeros((self._batch_size,vocabulary_size),dtype=np.float32)\n",
    "        \n",
    "        # Заполнение пакета\n",
    "        for b in range(self._batch_size):\n",
    "            # Сбрасывание курсора в начало сегмента,\n",
    "            # если курсор данного сегмента превышает длину сегмента\n",
    "            if self._cursor[b]+1>=self._text_size:\n",
    "                self._cursor[b] = b * self._segments\n",
    "            \n",
    "            # Добавление биграммы в курсор \n",
    "            batch_data[b,self._text[self._cursor[b]]] = 1.0\n",
    "            # Добавление предыдущей биграммы в качестве метки для предсказания\n",
    "            batch_labels[b,self._text[self._cursor[b]+1]]= 1.0                       \n",
    "            # Обновление положения курсора\n",
    "            self._cursor[b] = (self._cursor[b]+1)%self._text_size\n",
    "                    \n",
    "        return batch_data,batch_labels\n",
    "        \n",
    "    def unroll_batches(self):\n",
    "        '''\n",
    "        Получение листа num_unroll пакетов, требуемого для одного шага обучения\n",
    "        '''\n",
    "        unroll_data,unroll_labels = [],[]\n",
    "        for ui in range(self._num_unroll):\n",
    "            data, labels = self.next_batch()            \n",
    "            unroll_data.append(data)\n",
    "            unroll_labels.append(labels)\n",
    "        \n",
    "        return unroll_data, unroll_labels\n",
    "    \n",
    "    def reset_indices(self):\n",
    "        '''\n",
    "        Сбрасывание всех курсоров при необходимости\n",
    "        '''\n",
    "        self._cursor = [offset * self._segments for offset in range(self._batch_size)]\n",
    "        \n",
    "# Запуск небольшого набора\n",
    "dg = DataGeneratorOHE(data_list[0][25:50],5,5)\n",
    "u_data, u_labels = dg.unroll_batches()\n",
    "\n",
    "# Результат для каждого пакета данных\n",
    "for ui,(dat,lbl) in enumerate(zip(u_data,u_labels)):   \n",
    "    print('\\n\\nUnrolled index %d'%ui)\n",
    "    dat_ind = np.argmax(dat,axis=1)\n",
    "    lbl_ind = np.argmax(lbl,axis=1)\n",
    "    print('\\tInputs:')\n",
    "    for sing_dat in dat_ind:\n",
    "        print('\\t%s (%d)'%(reverse_dictionary[sing_dat],sing_dat),end=\", \")\n",
    "    print('\\n\\tOutput:')\n",
    "    for sing_lbl in lbl_ind:        \n",
    "        print('\\t%s (%d)'%(reverse_dictionary[sing_lbl],sing_lbl),end=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Количество предыдущих шагов, для которых развернут вход\n",
    "num_unroll = 50\n",
    "# Размер пакета\n",
    "batch_size = 64\n",
    "test_batch_size = 1\n",
    "# Размерность скрытых слоев\n",
    "hidden = 64\n",
    "# Размерность входных и выходных слоев\n",
    "in_size, out_size = vocabulary_size, vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Входные и выходные тренировочные данные\n",
    "train_dataset, train_labels = [], []\n",
    "for ui in range(num_unroll):\n",
    "    train_dataset.append(tf.placeholder(tf.float32, shape=[batch_size, in_size], name='train_dataset_%d'%ui))\n",
    "    train_labels.append(tf.placeholder(tf.float32, shape=[batch_size, out_size], name='train_labels_%d'%ui))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Входные и выходные данные для валидации\n",
    "valid_dataset = tf.placeholder(tf.float32, shape=[1, in_size], name='valid_dataset')\n",
    "valid_labels = tf.placeholder(tf.float32, shape=[1, out_size], name='valid_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тестовые входные данные\n",
    "test_dataset = tf.placeholder(tf.float32, shape=[test_batch_size, in_size], name='test_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Веса между входными данными и скрытым слоем\n",
    "W_xh = tf.Variable(tf.truncated_normal([in_size, hidden], stddev=0.02, dtype=tf.float32), name='W_xh')\n",
    "# Веса рекуррентных связей скрытого слоя\n",
    "W_hh = tf.Variable(tf.truncated_normal([hidden, hidden], stddev=0.02, dtype=tf.float32), name='W_hh')\n",
    "# Веса между скрытым слоем и выходными данными\n",
    "W_hy = tf.Variable(tf.truncated_normal([hidden, out_size], stddev=0.02, dtype=tf.float32), name='W_hy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Переменная состояния обучения\n",
    "prev_train_h = tf.Variable(tf.zeros([batch_size, hidden], dtype=tf.float32), name='train_h', trainable=False)\n",
    "# Переменная состояния проверки\n",
    "prev_valid_h = tf.Variable(tf.zeros([1, hidden], dtype=tf.float32), name='valid_h', trainable=False)\n",
    "# Переменная состояния проверки\n",
    "prev_test_h = tf.Variable(tf.zeros([test_batch_size, hidden], dtype=tf.float32), name='test_h',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тренировочные оценки и прогнозы\n",
    "y_scores, y_predictions = [], []\n",
    "\n",
    "# Присоединение вычисленного выхода RNN для каждого шага из num_unroll шагов\n",
    "outputs = list()\n",
    "\n",
    "# Шаг выхода RNN\n",
    "output_h = prev_train_h\n",
    "\n",
    "# Вычисление выхода RNN для num_unroll шагов вычислений\n",
    "for ui in range(num_unroll):\n",
    "    output_h = tf.nn.tanh(tf.matmul(tf.concat([train_dataset[ui], output_h], 1), tf.concat([W_xh, W_hh], 0)))\n",
    "    outputs.append(output_h)\n",
    "\n",
    "# Вычисление оценок и прогнозов для всех выходов RNN, которые имеются для num_unroll шагов\n",
    "y_scores = [tf.matmul(outputs[ui], W_hy) for ui in range(num_unroll)]\n",
    "y_predictions = [tf.nn.softmax(y_scores[ui]) for ui in range(num_unroll)]\n",
    "\n",
    "# Перплексия для учебного набора данных\n",
    "train_perplexity_without_exp = tf.reduce_sum(tf.concat(train_labels, 0)*-tf.log(tf.concat(y_predictions, 0)+1e-10))/(num_unroll * batch_size)\n",
    "\n",
    "# Следующее состояние для 1 шага\n",
    "next_valid_state = tf.nn.tanh(tf.matmul(valid_dataset, W_xh) + tf.matmul(prev_valid_h, W_hh))\n",
    "\n",
    "# Вычисление оценки и прогноза, используя выход состояния RNN, \n",
    "# с присваиванием последнего выхода состояния RNN переменной состояния при фазе валидации \n",
    "with tf.control_dependencies([tf.assign(prev_valid_h, next_valid_state)]):\n",
    "    valid_scores = tf.matmul(next_valid_state, W_hy)\n",
    "    valid_predictions = tf.nn.softmax(valid_scores)\n",
    "\n",
    "# Перплексия набора данных для проверки\n",
    "valid_perplexity_without_exp = tf.reduce_sum(valid_labels*-tf.log(valid_predictions+1e-10))\n",
    "\n",
    "# Следующее состояние для тестовых данных\n",
    "next_test_state = tf.nn.tanh(tf.matmul(test_dataset, W_xh) + tf.matmul(prev_test_h, W_hh))\n",
    "\n",
    "# Вычисление прогноза, используя выход состояния RNN, \n",
    "# с присваиванием последнего выхода состояния RNN переменной состояния при фазе валидации \n",
    "with tf.control_dependencies([tf.assign(prev_test_h, next_test_state)]):\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(next_test_state, W_hy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Присваивание переменной состояния значени\\ последнего выхода RNN\n",
    "with tf.control_dependencies([tf.assign(prev_train_h, output_h)]):\n",
    "    # Вычисление перекрестной энтропии для всех прогнозов, полученных за num_unroll шагов\n",
    "    rnn_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits=tf.concat(y_scores, 0), labels=tf.concat(train_labels, 0)\n",
    "    ))\n",
    "    \n",
    "# Функция потерь валидации\n",
    "rnn_valid_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits=valid_scores, labels=valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция оптимизации\n",
    "rnn_optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "\n",
    "# Оптимизация с отсечением градиентов\n",
    "gradients, v = zip(*rnn_optimizer.compute_gradients(rnn_loss))\n",
    "gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "rnn_optimizer = rnn_optimizer.apply_gradients(zip(gradients, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сброс скрытых состояний\n",
    "reset_train_h_op = tf.assign(prev_train_h, tf.zeros([batch_size, hidden], dtype=tf.float32))\n",
    "reset_valid_h_op = tf.assign(prev_valid_h, tf.zeros([1, hidden], dtype=tf.float32))\n",
    "reset_test_h_op = tf.assign(prev_test_h, tf.truncated_normal([test_batch_size, hidden], stddev=0.01, dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(distribution):\n",
    "    # Выбор слово из распределения предсказаний\n",
    "    best_idx = np.argmax(distribution)\n",
    "    return best_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Число шагов прогона алгоритма\n",
    "num_steps = 26\n",
    "# Число шагов тренировки каждого документа для одного шага\n",
    "steps_per_document = 100\n",
    "\n",
    "# Число валидаций\n",
    "valid_summary = 1\n",
    "\n",
    "# Число документов для тренировки (может принимать значения 20 или 100)\n",
    "train_doc_count = 20\n",
    "# Число документов, используемых на одиночном шаге\n",
    "# Когда train_doc_count = 20 => train_docs_to_use = 5\n",
    "# Когда train_doc_count = 100 => train_docs_to_use = 10\n",
    "train_docs_to_use = 5\n",
    "\n",
    "# Значения перплексии на каждом шаге\n",
    "valid_perplexity_ot = []\n",
    "train_perplexity_ot = []\n",
    "\n",
    "session = tf.InteractiveSession()\n",
    "# Объявление переменных\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "print('Initialized')\n",
    "\n",
    "average_loss = 0\n",
    "\n",
    "# Поиск первых 10 документов, которые содержат (num_steps + 1) * steps_per_document биграмм для создания набора данных валидации\n",
    "long_doc_ids = []\n",
    "for di in range(num_files):\n",
    "    if len(data_list[di]) > (num_steps + 1) * steps_per_document:\n",
    "        long_doc_ids.append(di)\n",
    "    if len(long_doc_ids) == 10:\n",
    "        break\n",
    "\n",
    "data_gens = []\n",
    "valid_gens = []\n",
    "for fi in range(num_files):\n",
    "    # Получение всех биграмм, если id документа нет в листе валидации id документов\n",
    "    if fi not in long_doc_ids:\n",
    "        data_gens.append(DataGeneratorOHE(data_list[fi], batch_size, num_unroll))\n",
    "    # Получение последних steps_per_document биграмм в качестве данных валидации,\n",
    "    # если документ есть в листе валидации id документов\n",
    "    else:\n",
    "        data_gens.append(DataGeneratorOHE(data_list[fi][:-steps_per_document], batch_size, num_unroll))\n",
    "        # Получение данных валидации генератора\n",
    "        valid_gens.append(DataGeneratorOHE(data_list[fi][:-steps_per_document], 1, 1))\n",
    "\n",
    "feed_dict = {}\n",
    "for step in range(num_steps):\n",
    "    print('\\n')\n",
    "    for di in np.random.permutation(train_doc_count)[:train_docs_to_use]:\n",
    "        doc_perplexity = 0\n",
    "        for doc_step_id in range(steps_per_document):\n",
    "            \n",
    "            # Получение набора развернутого пакета данных\n",
    "            u_data, u_labels = data_gens[di].unroll_batches()\n",
    "            \n",
    "            for ui, (dat, lbl) in enumerate(zip(u_data, u_labels)):\n",
    "                feed_dict[train_dataset[ui]] = dat\n",
    "                feed_dict[train_labels[ui]] = lbl\n",
    "                \n",
    "            _, l, step_predictions, _, step_labels, step_perplexity = \\\n",
    "            session.run([rnn_optimizer, rnn_loss, y_predictions,\n",
    "                        train_dataset, train_labels, train_perplexity_without_exp],\n",
    "                        feed_dict=feed_dict)\n",
    "            \n",
    "            # Обновление значения перплексии документа\n",
    "            doc_perplexity += step_perplexity\n",
    "            # Обновление значения средней потери\n",
    "            average_loss += step_perplexity\n",
    "            \n",
    "        print('Document %d Step %d processed (Perplexity = %.2f)' \n",
    "              %(di, step + 1, np.exp(doc_perplexity / steps_per_document)))\n",
    "        \n",
    "        session.run(reset_train_h_op)\n",
    "    \n",
    "    if step % valid_summary == 0:\n",
    "        \n",
    "        # Вычисление значения средней потери\n",
    "        average_loss = average_loss / (train_docs_to_use * steps_per_document * valid_summary)\n",
    "        \n",
    "        print('Avarage loss at step %d: %f' %(step + 1, average_loss))\n",
    "        print('\\tPerplexity at step %d: %f' %(step + 1, np.exp(average_loss)))\n",
    "        train_perplexity_ot.append(np.exp(average_loss))\n",
    "        \n",
    "        # Сбрасывание значения средней потери\n",
    "        average_loss = 0\n",
    "        # Сбрасывание значения потери валидации\n",
    "        valid_loss = 0\n",
    "        \n",
    "        for v_doc_id in range(10):\n",
    "            for v_step in range(steps_per_document // 2):\n",
    "                uvalid_data, uvalid_labels = valid_gens[v_doc_id].unroll_batches()\n",
    "                \n",
    "                v_perp = session.run(valid_perplexity_without_exp, \n",
    "                                    feed_dict={\n",
    "                                        valid_dataset : uvalid_data[0],\n",
    "                                        valid_labels : uvalid_labels[0]\n",
    "                                    })\n",
    "                \n",
    "                valid_loss += v_perp\n",
    "            \n",
    "            session.run(reset_valid_h_op)\n",
    "            # Сбрасывание курсора генератора валидационных данных\n",
    "            valid_gens[v_doc_id].reset_indices()\n",
    "        \n",
    "        print()\n",
    "        v_perplexity = np.exp(valid_loss / (steps_per_document * 10.0 // 2))\n",
    "        print(\"Valid Perplexity = %.2f\\n\" %v_perplexity)\n",
    "        valid_perplexity_ot.append(v_perplexity)\n",
    "        \n",
    "        print('Generated text after epoch %d ...' %step)\n",
    "        segments_to_generate = 1\n",
    "        chars_in_segment = 1000\n",
    "        \n",
    "        for _ in range(segments_to_generate):\n",
    "            print('======================== New text Segment ==========================')\n",
    "            # Начало со случайного слова\n",
    "            test_word = np.zeros((1, in_size), dtype=np.float32)\n",
    "            test_word[0, data_list[np.random.randint(0, num_files)][np.random.randint(0, 100)]] = 1.0\n",
    "            print(\"\\t\", reverse_dictionary[np.argmax(test_word[0])], end='')\n",
    "            \n",
    "            for _ in range(chars_in_segment):\n",
    "                test_pred = session.run(test_prediction, feed_dict = {test_dataset : test_word})\n",
    "                next_ind = sample(test_pred.ravel())\n",
    "                test_word = np.zeros((1, in_size), dtype=np.float32)\n",
    "                test_word[0, next_ind] = 1.0\n",
    "                print(reverse_dictionary[next_ind], end='')\n",
    "                \n",
    "            print(\"\")\n",
    "            # Обновление тестового состояния\n",
    "            session.run(reset_test_h_op)\n",
    "            print('====================================================================')\n",
    "        print(\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Построение перплексии RNN\n",
    "x_axis = np.arange(len(train_perplexity_ot[1:25]))\n",
    "f, (ax1,ax2) = pylab.subplots(1, 2, figsize=(18,6))\n",
    "\n",
    "ax1.plot(x_axis, train_perplexity_ot[1:25], label='Train')\n",
    "ax2.plot(x_axis, valid_perplexity_ot[1:25], label='Valid')\n",
    "\n",
    "pylab.title('Train and Valid Perplexity over Time', fontsize=24)\n",
    "ax1.set_title('Train Perplexity', fontsize=20)\n",
    "ax2.set_title('Valid Perplexity', fontsize=20)\n",
    "ax1.set_xlabel('Epoch', fontsize=20)\n",
    "ax2.set_xlabel('Epoch', fontsize=20)\n",
    "pylab.savefig('RNN_perplexity.png')\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
